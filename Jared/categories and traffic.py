# -*- coding: utf-8 -*-
"""MSBA Capstone 2 (JR).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BrQofV-hd327Mgab2Fmx2d6aIrch6YA5

This script reads a CSV file containing URLs, extracts unique URLs, and queries the Klazify API to categorize each URL based on its content. It then stores the category and confidence score in a dictionary, maps the results back to the original dataset, and saves the updated data to a new CSV file. The script also includes error handling and API rate limiting to ensure smooth execution.
"""

import pandas as pd
import requests
import time

# Your Klazify API key
API_KEY = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJhdWQiOiIxIiwianRpIjoiMjgxZTYyYjAxMmQ3NjFmN2E4YTRkZTIwOTJhM2QzMDhlNTNkZTFmMmQ1ZTI0ZmU4NGJjYzNmMTc2OTExODQ2ZmFmMjljNzRhNTE4ODBiZGQiLCJpYXQiOjE3Mzk0NjIwNjYuOTg4Mjc3LCJuYmYiOjE3Mzk0NjIwNjYuOTg4Mjc5LCJleHAiOjE3NzA5OTgwNjYuOTgyODE0LCJzdWIiOiIxMzgwMyIsInNjb3BlcyI6W119.xMIkQXy54Es4FzYMJNrUbw_58FeZ-5gnXhtY5xjVcra8dfzvo1f_-02CXcExjoKRRD5kPJcx10stCmi-7b_pTg6Mm9UTlC0YnX_Dlmx6-tOw9RwCXII3NmB3j7URlR3YkikGJEjYQQ8dkkwDGxM9ysPBkdKKJZbfy_prktnD1hHQX6dsLaqQQuuAltr8I6Weh1M1VM3-KSnM2h3i7JoEyQnPyOB5pUPyVU6ohfmuAkYrT7aigvlXh41Brau5iDB6VZ8Dm-RDkJOPEmJ7m6RCnM_Z59rVc6iVAOsaM1sjHd5uPet3EGoQ-jJbi_Y3pSzeBYaeEJ7nkoG6Wv8-J2C4NIbPfN_6Hx_qkcZqktgmruqMxU5LdyAI3NeVhexMjQpB5-xrHEO1-moiEyxF2Ways81-Voi9scvZxTWnkngHgzK6FVowiEvOBhKFqzryb0WoVsi9FAvldsSLdozWEktukq935E53m3fkGUlNI0i5zaWIB3QB3ZrOw7Wy2lFEiplDpteHml_ST3mVFqDiZa-tPuKhujzarS_5v6Q2HVGZj86daNRk_Pi-A5ikiaj4gQfM5wm21889iBSp5Rl4F79bMo2q7szvh26Fhf1PDxR74xXl8E5w_R_aTb-QTb-QFd2Ycrwqvcb59G_FwD0ZjHtgNfcVe-ZdfhoeumsfSWuFmhQ"  # Replace with your actual API key

# Endpoint URL
API_URL = "https://www.klazify.com/api/categorize"

# Load your CSV file
df = pd.read_csv("backlinks_results.csv")

# Extract unique 'URL_To' entries
unique_urls = df["URL_To"].unique()

# Set up headers
headers = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json"
}

# Dictionary to store API results
category_results = {}

def categorize_url(url):
    """Fetch category information for a given URL."""
    payload = {"url": url}
    response = requests.post(API_URL, json=payload, headers=headers)

    if response.status_code == 200:
        data = response.json()
        categories = data.get("domain", {}).get("categories", [])

        if categories:
            top_category = categories[0]  # Take the first (most relevant) category
            return top_category.get("name"), top_category.get("confidence")
        else:
            return None, None
    else:
        print(f"âŒ Request failed for {url} with status code {response.status_code}")
        return None, None

# Loop through unique URLs and get category data
for i, url in enumerate(unique_urls):
    category_name, confidence = categorize_url(url)
    category_results[url] = (category_name, confidence)

    print(f"âœ… Processed {i+1}/{len(unique_urls)}: {url} -> {category_name} (Confidence: {confidence})")

    # Respect API rate limits (adjust if necessary)
    time.sleep(1)

# Map results back to the original DataFrame
df["category_name"] = df["URL_To"].map(lambda x: category_results.get(x, (None, None))[0])
df["category_confidence"] = df["URL_To"].map(lambda x: category_results.get(x, (None, None))[1])

# Save the updated file
df.to_csv("backlinks_results_with_categories.csv", index=False)

print("ðŸŽ‰ Done! Updated CSV file is saved as 'backlinks_results_with_categories.csv'.")

"""This script uploads a CSV file containing URLs, extracts unique URLs from the URL_To column, and queries the DataForSEO API to retrieve organic and paid traffic estimates for each URL. It stores the results in a dictionary, maps them back to the dataset, and saves the updated file as backlinks_results_with_categories_and_traffic.csv. The script also includes error handling and rate limiting to ensure efficient API usage."""

import pandas as pd
from google.colab import files
from client import RestClient
import time

# Step 1: Upload CSV file
print("Please upload the 'backlinks_results_with_categories.csv' file:")
uploaded = files.upload()  # Prompt user to upload file

# Step 2: Read the uploaded CSV file
file_name = list(uploaded.keys())[0]  # Get uploaded file name
data = pd.read_csv(file_name)

# Step 3: Debugging: Print column names
print("Columns in the uploaded CSV file:")
print(data.columns)

# Step 4: Ensure the correct column name for 'URL_To'
column_name = "URL_To"
if column_name not in data.columns:
    raise KeyError(f"Column '{column_name}' not found in the CSV file. Available columns: {list(data.columns)}")

# Extract unique URLs from 'URL_To' column
unique_urls = data[column_name].unique()

# Initialize RestClient with your credentials
client = RestClient("jr4505@stern.nyu.edu", "a25812166edbfe67")

# Dictionary to store traffic results for unique URLs
traffic_results = {}

# Step 5: Loop through unique URLs and make API calls
for index, url in enumerate(unique_urls):
    print(f"Processing {index + 1}/{len(unique_urls)}: {url}")

    # Prepare the API request payload
    post_data = {
        "0": {
            "targets": [url],
            "date_from": "2022-12-05",
            "date_to": "2024-12-04",
            "item_types": ["organic", "paid"]
        }
    }

    # Make the API request
    response = client.post("/v3/dataforseo_labs/google/bulk_traffic_estimation/live", post_data)

    if response["status_code"] != 20000:
        print(f"âŒ Error for {url}: Code {response['status_code']}, Message: {response['status_message']}")
        traffic_results[url] = (0, 0)
        continue

    # Extract results
    organic_traffic, paid_traffic = 0, 0  # Default values
    tasks = response.get("tasks", [])

    for task in tasks:
        results = task.get("result", [])
        for result in results:
            items = result.get("items", [])
            for item in items:
                organic_traffic = item.get("metrics", {}).get("organic", {}).get("etv", 0)
                paid_traffic = item.get("metrics", {}).get("paid", {}).get("etv", 0)

    # Store results in dictionary
    traffic_results[url] = (organic_traffic, paid_traffic)

    print(f"âœ… {url} -> Organic: {organic_traffic}, Paid: {paid_traffic}")

    # Respect API rate limits
    time.sleep(1)

# Step 6: Map traffic data back to the full dataset
data["Organic"] = data["URL_To"].map(lambda x: traffic_results.get(x, (0, 0))[0])
data["Paid"] = data["URL_To"].map(lambda x: traffic_results.get(x, (0, 0))[1])

# Step 7: Save the updated DataFrame
output_file = "backlinks_results_with_categories_and_traffic.csv"
data.to_csv(output_file, index=False)

print(f"ðŸŽ‰ Updated traffic data saved to {output_file}")

"""This script trains an optimized Random Forest model to predict log-transformed organic traffic based on URL features and category groupings. It starts by uploading and preprocessing the dataset, including categorizing industries, extracting SEO-related features (domain length, HTTPS status, presence of numbers), and hyperparameter tuning using GridSearchCV to find the best model. Finally, it trains the best model, evaluates performance (RÂ², MAE), and prints feature importance, helping identify the key drivers of organic traffic."""

# Import necessary libraries
import pandas as pd
import numpy as np
import re
from sklearn.ensemble import RandomForestRegressor  # Importing Random Forest
from sklearn.model_selection import train_test_split, GridSearchCV  # Splitting data & hyperparameter tuning
from sklearn.metrics import r2_score, mean_absolute_error  # Model evaluation metrics
from google.colab import files  # For uploading files in Google Colab

# Step 1: Upload CSV file in Google Colab
print("Please upload 'backlinks_results_with_categories_and_traffic.csv':")
uploaded = files.upload()  # Prompts user to upload the CSV file

# Step 2: Read the uploaded CSV file into a Pandas DataFrame
file_name = list(uploaded.keys())[0]  # Get the uploaded file name
df = pd.read_csv(file_name)  # Read CSV into DataFrame

# Step 3: Ensure numeric data & drop missing values
df['Organic'] = pd.to_numeric(df['Organic'], errors='coerce')  # Convert 'Organic' column to numeric
df['Paid'] = pd.to_numeric(df['Paid'], errors='coerce')  # Convert 'Paid' column to numeric
df = df[['category_name', 'Organic', 'Paid', 'URL_To']].dropna()  # Keep only relevant columns and drop NaN values

# Step 4: Apply log transformation to 'Organic' traffic
df['log_organic'] = np.log1p(df['Organic'])  # log1p(x) = log(x + 1) to handle zeros

# Step 5: Function to simplify 'category_name' into broader topics
def simplify_category(category):
    """Groups categories into broader topics to reduce the number of unique labels."""
    if "Computers & Electronics" in category:
        return "Technology"
    elif "Finance" in category:
        return "Finance"
    elif "Health" in category:
        return "Health"
    elif "Business & Industrial" in category:
        return "Business"
    elif "Internet & Telecom" in category:
        return "Internet"
    elif "Science" in category:
        return "Science"
    elif "Education" in category or "Jobs & Education" in category:
        return "Education"
    else:
        return "Other"

# Apply the function to create a new column
df['category_group'] = df['category_name'].apply(simplify_category)

# Step 6: Convert categorical variable 'category_group' into numerical format (One-Hot Encoding)
df_encoded = pd.get_dummies(df, columns=['category_group'], drop_first=True, dtype=float)

# Step 7: Extract additional features from the URL
df_encoded['domain_length'] = df['URL_To'].apply(lambda x: len(str(x)))  # Measure length of domain
df_encoded['has_numbers'] = df['URL_To'].apply(lambda x: int(bool(re.search(r'\d', str(x)))))  # Check if URL contains numbers
df_encoded['is_https'] = df['URL_To'].apply(lambda x: 1 if str(x).startswith("https") else 0)  # Check if the URL uses HTTPS

# Step 8: Prepare training data
X = df_encoded.drop(columns=['Organic', 'log_organic', 'URL_To', 'category_name'])  # Drop unnecessary columns
y = df_encoded['log_organic']  # Define target variable (log-transformed organic traffic)

# Step 9: Split data into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 10: Hyperparameter tuning for Random Forest using GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees in the forest
    'max_depth': [5, 10, 20, None],  # Depth of trees (None means unlimited)
    'min_samples_split': [2, 5, 10]  # Minimum samples required to split a node
}

rf = RandomForestRegressor(random_state=42)  # Initialize the Random Forest model
grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='r2', n_jobs=-1)  # Perform cross-validation to find best parameters
grid_search.fit(X_train, y_train)  # Fit the model to training data

best_rf = grid_search.best_estimator_  # Get the best model from Grid Search

# Step 11: Train the optimized Random Forest model on full training data
best_rf.fit(X_train, y_train)

# Step 12: Make predictions on the test set
y_pred_rf = best_rf.predict(X_test)

# Step 13: Evaluate Model Performance
r2_rf = r2_score(y_test, y_pred_rf)  # RÂ² Score (How well the model fits)
mae_rf = mean_absolute_error(y_test, y_pred_rf)  # Mean Absolute Error (Average prediction error)

# Print the evaluation results
print(f"âœ… Optimized Random Forest RÂ² Score: {r2_rf:.4f}")  # Higher is better (closer to 1)
print(f"âœ… Optimized Random Forest MAE: {mae_rf:.4f}")  # Lower is better

# Step 14: Analyze Feature Importance
feature_importance_rf = pd.Series(best_rf.feature_importances_, index=X.columns).sort_values(ascending=False)

# Print feature importance values
print("\nðŸ“Š Random Forest Feature Importance:")
print(feature_importance_rf)