# -*- coding: utf-8 -*-
"""(a).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zJU-o8MyNydY-nZ2zZwq5o4F1V-BJXib
"""

import requests
import pandas as pd
from requests.auth import HTTPBasicAuth

# API Credentials (replace with your credentials)
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"

# API Endpoint
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# Target domains
target_domains = ["covertswarm.com", "apple.com", "google.com"]

# Headers for the API request
HEADERS = {
    "Content-Type": "application/json"
}

# Function to fetch keyword info for a domain
def fetch_keyword_info(domain):
    # Wrap the task in an array
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English"
        }
    ]
    response = requests.post(API_ENDPOINT, headers=HEADERS, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging: Print full response
        if data.get("status_code") == 20000:
            keyword_info_list = []
            for task in data.get("tasks", []):
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        keyword_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", ""),
                            "se_type": result.get("se_type", ""),
                            "last_updated_time": result.get("last_updated_time", ""),
                            "competition": item.get("competition", "N/A"),
                            "competition_level": item.get("competition_level", "UNKNOWN"),
                            "cpc": item.get("cpc", 0.0),
                            "search_volume": item.get("search_volume", 0),
                            "low_top_of_page_bid": item.get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("high_top_of_page_bid", 0.0),
                            "categories": item.get("categories", []),
                            "monthly_searches": item.get("monthly_searches", [])
                        }
                        keyword_info_list.append(keyword_info)
            # Sort by search volume in ascending order and pick top 3
            keyword_info_list = sorted(keyword_info_list, key=lambda x: x["search_volume"])[:3]
            return keyword_info_list
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")
    return []


# Fetch and compile keyword data for all domains
all_keywords_info = []
for domain in target_domains:
    print(f"Fetching keywords for {domain}...")
    keyword_info = fetch_keyword_info(domain)
    all_keywords_info.extend(keyword_info)

# Convert results to a DataFrame
df = pd.DataFrame(all_keywords_info)

# Save to CSV
output_file = "top_3_keywords_per_domain.csv"
df.to_csv(output_file, index=False)
print(f"Keyword information saved to {output_file}")

# Display the DataFrame
print(df)

import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import time

# API credentials
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# Domains to query
domains = ["covertswarm.com", "apple.com", "google.com"]

# Initialize a list to store keyword info
all_keywords_data = []

# Fetch data for each domain
for domain in domains:
    print(f"Fetching data for {domain}...")

    # Payload for the API request
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English"
        }
    ]

    # Make the API request
    response = requests.post(API_ENDPOINT, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging: Print the full response

        if data.get("status_code") == 20000:
            for task in data.get("tasks", []):
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        # Extract the required fields
                        keyword_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", "N/A"),
                            "se_type": result.get("se_type", "N/A"),
                            "last_updated_time": result.get("last_updated_time", "N/A"),
                            "competition": item.get("competition", "N/A"),
                            "competition_level": item.get("competition_level", "UNKNOWN"),
                            "cpc": item.get("cpc", 0.0),
                            "search_volume": item.get("search_volume", 0),
                            "low_top_of_page_bid": item.get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("high_top_of_page_bid", 0.0),
                            "categories": item.get("categories", []),
                            "monthly_searches": item.get("monthly_searches", [])
                        }
                        all_keywords_data.append(keyword_info)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Pause between requests to avoid hitting rate limits
    time.sleep(2)

# Save data to a CSV file
if all_keywords_data:
    df = pd.DataFrame(all_keywords_data)
    csv_filename = "keyword_search_volume.csv"
    df.to_csv(csv_filename, index=False)
    print(f"Keyword information saved to {csv_filename}")
else:
    print("No data retrieved.")

import requests
from requests.auth import HTTPBasicAuth
import time
import pandas as pd

API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# 도메인 리스트
domains = ["covertswarm.com", "apple.com", "google.com"]

# 결과 저장 리스트
all_results = []

for domain in domains:
    print(f"Fetching data for {domain}...")

    # 요청 payload 생성
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English"
        }
    ]

    # API 호출
    response = requests.post(API_ENDPOINT, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    # HTTP 상태 코드 확인
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # 응답 디버깅

        if data.get("status_code") == 20000:  # 성공 여부 확인
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        # 필요한 데이터 추출
                        keyword_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", ""),
                            "search_volume": item.get("keyword_info", {}).get("search_volume", 0),
                            "competition_level": item.get("keyword_info", {}).get("competition_level", "UNKNOWN"),
                            "cpc": item.get("keyword_info", {}).get("cpc", 0),
                            "low_top_of_page_bid": item.get("keyword_info", {}).get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("keyword_info", {}).get("high_top_of_page_bid", 0.0),
                            "categories": item.get("keyword_info", {}).get("categories", []),
                            "monthly_searches": item.get("keyword_info", {}).get("monthly_searches", []),
                        }
                        all_results.append(keyword_info)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # **API 호출 사이에 1초 지연 추가**
    time.sleep(1)

# 결과를 데이터프레임으로 변환
df = pd.DataFrame(all_results)

# CSV 파일로 저장
csv_filename = "keyword_search_volume.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword information saved to {csv_filename}")

import requests
from requests.auth import HTTPBasicAuth
import time
import pandas as pd

API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# Domain list
domains = ["redscan.com", "tekkis.com", "ek.co"]

# Date range filter
start_year = 2022
start_month = 12
start_day = 5
end_year = 2024
end_month = 12
end_day = 4

# Results storage
all_results = []

for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create payload
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English"
        }
    ]

    # API request
    response = requests.post(API_ENDPOINT, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    # Check HTTP status code
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        if data.get("status_code") == 20000:  # Check success
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        # Extract base keyword info
                        base_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", ""),
                            "last_updated_time": result.get("last_updated_time", "N/A"),
                            "previous_updated_time": result.get("previous_updated_time", "N/A"),  # Add previous_updated_time
                            "competition": item.get("keyword_info", {}).get("competition", "N/A"),
                            "competition_level": item.get("keyword_info", {}).get("competition_level", "UNKNOWN"),
                            "cpc": item.get("keyword_info", {}).get("cpc", 0),
                            "low_top_of_page_bid": item.get("keyword_info", {}).get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("keyword_info", {}).get("high_top_of_page_bid", 0.0),
                            "categories": item.get("keyword_info", {}).get("categories", [])
                        }

                        # Filter monthly searches within the specified date range
                        for monthly_data in item.get("keyword_info", {}).get("monthly_searches", []):
                            year = monthly_data.get("year")
                            month = monthly_data.get("month")
                            search_volume = monthly_data.get("search_volume", 0)

                            # Check if the date falls within the desired range
                            if (year > start_year or (year == start_year and month >= start_month)) and \
                               (year < end_year or (year == end_year and month <= end_month)):
                                expanded_info = base_info.copy()
                                expanded_info["year"] = year
                                expanded_info["month"] = month
                                expanded_info["search_volume"] = search_volume
                                all_results.append(expanded_info)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Add delay to avoid hitting API rate limits
    time.sleep(1)

# Convert results to a DataFrame
df = pd.DataFrame(all_results)

# Save to CSV file
csv_filename = "keyword_search_volume_with_update_times.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword information saved to {csv_filename}")

#Final Ver : last update, date range cant not be setted.
import requests
from requests.auth import HTTPBasicAuth
import time
import pandas as pd

API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# Domain list
domains = ["redscan.com", "tekkis.com", "ek.co"]

# Date range filter
start_year = 2022
start_month = 12
start_day = 5
end_year = 2024
end_month = 12
end_day = 4

# Results storage
all_results = []

for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create payload
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English",
            "include_serp_info": True,  # Ensure SERP information is included
            "include_subdomains": True  # Include subdomains if applicable
        }
    ]

    # API request
    response = requests.post(API_ENDPOINT, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    # Check HTTP status code
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        if data.get("status_code") == 20000:  # Check success
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        # Extract base keyword info
                        base_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", ""),
                            "last_updated_time": result.get("last_updated_time", "N/A"),
                            "previous_updated_time": result.get("previous_updated_time", "N/A"),  # Add previous_updated_time
                            "competition": item.get("keyword_info", {}).get("competition", "N/A"),
                            "competition_level": item.get("keyword_info", {}).get("competition_level", "UNKNOWN"),
                            "cpc": item.get("keyword_info", {}).get("cpc", 0),
                            "low_top_of_page_bid": item.get("keyword_info", {}).get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("keyword_info", {}).get("high_top_of_page_bid", 0.0),
                            "categories": item.get("keyword_info", {}).get("categories", [])
                        }

                        # Filter monthly searches within the specified date range
                        for monthly_data in item.get("keyword_info", {}).get("monthly_searches", []):
                            year = monthly_data.get("year")
                            month = monthly_data.get("month")
                            search_volume = monthly_data.get("search_volume", 0)

                            # Check if the date falls within the desired range
                            if (year > start_year or (year == start_year and month >= start_month)) and \
                               (year < end_year or (year == end_year and month <= end_month)):
                                expanded_info = base_info.copy()
                                expanded_info["year"] = year
                                expanded_info["month"] = month
                                expanded_info["search_volume"] = search_volume
                                all_results.append(expanded_info)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Add delay to avoid hitting API rate limits
    time.sleep(1)

# Convert results to a DataFrame
df = pd.DataFrame(all_results)

# Save to CSV file
csv_filename = "keyword_search_volume_with_update_times.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword information saved to {csv_filename}")

import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import time

# API Credentials
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/keywords_for_site/live"

# Target domains
domains = ["redscan.com", "tekkis.com", "covertswarm.com"]

# List to store results
all_results = []

# Date range filter
start_year, start_month = 2021, 12
end_year, end_month = 2022, 12

# Loop through each domain to fetch data
for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create payload with necessary parameters
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English",
            "include_serp_info": True,  # Ensure SERP information is included
            "include_subdomains": True  # Include subdomains if applicable
        }
    ]

    # Send API request
    response = requests.post(
        API_ENDPOINT,
        json=payload,
        auth=HTTPBasicAuth(API_USER, API_PASSWORD)
    )

    # Check HTTP response status
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        if data.get("status_code") == 20000:  # Check if API request was successful
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    for item in result.get("items", []):
                        # Extract the required fields
                        keyword_info = {
                            "domain": domain,
                            "keyword": item.get("keyword", "N/A"),
                            "last_updated_time": result.get("last_updated_time", "N/A"),
                            "previous_updated_time": result.get("previous_updated_time", "N/A"),
                            "competition": item.get("keyword_info", {}).get("competition", 0.0),
                            "search_volume": item.get("keyword_info", {}).get("search_volume", 0),
                            "cpc": item.get("keyword_info", {}).get("cpc", 0.0),
                            "low_top_of_page_bid": item.get("keyword_info", {}).get("low_top_of_page_bid", 0.0),
                            "high_top_of_page_bid": item.get("keyword_info", {}).get("high_top_of_page_bid", 0.0),
                            "categories": item.get("keyword_info", {}).get("categories", []),
                            "monthly_searches": item.get("keyword_info", {}).get("monthly_searches", [])
                        }
                        all_results.append(keyword_info)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Pause between requests to avoid hitting API rate limits
    time.sleep(1)

# Convert results to a pandas DataFrame
df = pd.DataFrame(all_results)

# Expand monthly_searches and filter by date range
if not df.empty:
    monthly_data = []
    for _, row in df.iterrows():
        for month_data in row["monthly_searches"]:
            year, month = month_data["year"], month_data["month"]
            # Check if the date falls within the specified range
            if (start_year < year < end_year) or (
                (year == start_year and month >= start_month) or (year == end_year and month <= end_month)
            ):
                monthly_row = {
                    "domain": row["domain"],
                    "keyword": row["keyword"],
                    "last_updated_time": row["last_updated_time"],
                    "previous_updated_time": row["previous_updated_time"],
                    "competition": row["competition"],
                    "search_volume": row["search_volume"],
                    "cpc": row["cpc"],
                    "low_top_of_page_bid": row["low_top_of_page_bid"],
                    "high_top_of_page_bid": row["high_top_of_page_bid"],
                    "categories": row["categories"],
                    "year": year,
                    "month": month,
                    "monthly_search_volume": month_data["search_volume"]
                }
                monthly_data.append(monthly_row)

    # Create expanded DataFrame with monthly search data
    df = pd.DataFrame(monthly_data)

# Save results to CSV
csv_filename = "keyword_data_with_date_filter.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword data saved to {csv_filename}")

!wget https://cdn.dataforseo.com/v3/examples/python/python_Client.zip
!unzip python_Client.zip

import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import time

# API Credentials
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/ranked_keywords/live"

# Target domains
domains = ["covertswarm.com", "redscan.com", "tekkis.com", "ek.co"]

# List to store results
all_results = []

# Loop through each domain to fetch data
for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create payload with necessary parameters
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English",
            "date_from": "2022-12-05",
            "date_to": "2024-12-04",
            "include_organic": True,
            "include_paid": True,
        }
    ]

    # Send API request
    response = requests.post(
        API_ENDPOINT,
        json=payload,
        auth=HTTPBasicAuth(API_USER, API_PASSWORD)
    )

    # Check HTTP response status
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        if data.get("status_code") == 20000:  # Check if API request was successful
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    metrics = result.get("metrics", {})
                    for item in result.get("items", []):
                        keyword_info = item.get("keyword_data", {})
                        keyword_details = {
                            "domain": domain,
                            "keyword": keyword_info.get("keyword", "N/A"),
                            "search_volume": keyword_info.get("keyword_info", {}).get("search_volume", 0),
                            "competition": keyword_info.get("keyword_info", {}).get("competition", "N/A"),
                            "competition_level": keyword_info.get("keyword_info", {}).get("competition_level", "N/A"),
                            "cpc": keyword_info.get("keyword_info", {}).get("cpc", 0.0),
                            "last_updated_time": keyword_info.get("keyword_info", {}).get("last_updated_time", "N/A"),
                            "organic_pos_1": metrics.get("organic", {}).get("pos_1", 0),
                            "paid_pos_1": metrics.get("paid", {}).get("pos_1", 0),
                            "organic_etv": metrics.get("organic", {}).get("etv", 0),
                            "paid_etv": metrics.get("paid", {}).get("etv", 0),
                        }

                        # Add monthly searches
                        monthly_searches = keyword_info.get("keyword_info", {}).get("monthly_searches", [])
                        for monthly_data in monthly_searches:
                            year_month = f"{monthly_data['year']}-{monthly_data['month']:02}"
                            keyword_details[year_month] = monthly_data["search_volume"]

                        all_results.append(keyword_details)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Pause between requests to avoid hitting API rate limits
    time.sleep(1)

# Convert results to a pandas DataFrame
df = pd.DataFrame(all_results)

# Save results to CSV
csv_filename = "ranked_keywords_with_traffic.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword data saved to {csv_filename}")

#Final Ver_Organic Traffic by KWs based on domains_But only the recent 12 months.
import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import time

# API Credentials
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/ranked_keywords/live"

# Target domains
domains = [ "covertswarm.com",
        "redscan.com",
        "tekkis.com",
        "ek.co",
        "approach-cyber.com",
        "coresecurity.com",
        "packetlabs.net",
        "purplesec.us",
        "zelvin.com",
        "breachlock.com",
        "hackerone.com",
        "offsec.com",
        "whiteknightlabs.com",
        "synack.com",
        "bishopfox.com",
        "mitnicksecurity.com",
        "tcm-sec.com",
        "coalfire.com",
        "dionach.com",
        "raxis.com"]

# List to store results
all_results = []

# Date range
date_from = "2022-12-05"
date_to = "2024-12-04"

# Loop through each domain to fetch data
for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create payload with necessary parameters
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English",
            "date_from": date_from,
            "date_to": date_to,
            "include_organic": True,
            "include_paid": True,
        }
    ]

    # Send API request
    response = requests.post(
        API_ENDPOINT,
        json=payload,
        auth=HTTPBasicAuth(API_USER, API_PASSWORD)
    )

    # Check HTTP response status
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        if data.get("status_code") == 20000:  # Check if API request was successful
            tasks = data.get("tasks", [])
            for task in tasks:
                for result in task.get("result", []):
                    metrics = result.get("metrics", {})
                    for item in result.get("items", []):
                        keyword_info = item.get("keyword_data", {})
                        keyword_details = {
                            "domain": domain,
                            "keyword": keyword_info.get("keyword", "N/A"),
                            "search_volume": keyword_info.get("keyword_info", {}).get("search_volume", 0),
                            "competition": keyword_info.get("keyword_info", {}).get("competition", "N/A"),
                            "competition_level": keyword_info.get("keyword_info", {}).get("competition_level", "N/A"),
                            "cpc": keyword_info.get("keyword_info", {}).get("cpc", 0.0),
                            "last_updated_time": keyword_info.get("keyword_info", {}).get("last_updated_time", "N/A"),
                            # Organic Positions
                            "organic_pos_1": metrics.get("organic", {}).get("pos_1", 0),
                            "organic_pos_2_3": metrics.get("organic", {}).get("pos_2_3", 0),
                            "organic_pos_4_10": metrics.get("organic", {}).get("pos_4_10", 0),
                            "organic_pos_11_20": metrics.get("organic", {}).get("pos_11_20", 0),
                            "organic_pos_21_30": metrics.get("organic", {}).get("pos_21_30", 0),
                            "organic_pos_31_40": metrics.get("organic", {}).get("pos_31_40", 0),
                            "organic_pos_41_50": metrics.get("organic", {}).get("pos_41_50", 0),
                            "organic_pos_51_60": metrics.get("organic", {}).get("pos_51_60", 0),
                            "organic_pos_61_70": metrics.get("organic", {}).get("pos_61_70", 0),
                            "organic_pos_71_80": metrics.get("organic", {}).get("pos_71_80", 0),
                            "organic_pos_81_90": metrics.get("organic", {}).get("pos_81_90", 0),
                            "organic_pos_91_100": metrics.get("organic", {}).get("pos_91_100", 0),
                            # Paid Positions
                            "paid_pos_1": metrics.get("paid", {}).get("pos_1", 0),
                            "paid_pos_2_3": metrics.get("paid", {}).get("pos_2_3", 0),
                            "paid_pos_4_10": metrics.get("paid", {}).get("pos_4_10", 0),
                            "paid_pos_11_20": metrics.get("paid", {}).get("pos_11_20", 0),
                            "paid_pos_21_30": metrics.get("paid", {}).get("pos_21_30", 0),
                            "paid_pos_31_40": metrics.get("paid", {}).get("pos_31_40", 0),
                            "paid_pos_41_50": metrics.get("paid", {}).get("pos_41_50", 0),
                            "paid_pos_51_60": metrics.get("paid", {}).get("pos_51_60", 0),
                            "paid_pos_61_70": metrics.get("paid", {}).get("pos_61_70", 0),
                            "paid_pos_71_80": metrics.get("paid", {}).get("pos_71_80", 0),
                            "paid_pos_81_90": metrics.get("paid", {}).get("pos_81_90", 0),
                            "paid_pos_91_100": metrics.get("paid", {}).get("pos_91_100", 0),
                            "organic_etv": metrics.get("organic", {}).get("etv", 0),
                            "paid_etv": metrics.get("paid", {}).get("etv", 0),
                        }

                        # Add monthly searches
                        monthly_searches = keyword_info.get("keyword_info", {}).get("monthly_searches", [])
                        for monthly_data in monthly_searches:
                            year_month = f"{monthly_data['year']}-{monthly_data['month']:02}"
                            keyword_details[year_month] = monthly_data["search_volume"]

                        all_results.append(keyword_details)
        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Pause between requests to avoid hitting API rate limits
    time.sleep(1)

# Convert results to a pandas DataFrame
df = pd.DataFrame(all_results)

# Save results to CSV
csv_filename = "ranked_keywords_with_traffic.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword data saved to {csv_filename}")

import requests
from requests.auth import HTTPBasicAuth
import pandas as pd
import time

# API Credentials
API_USER = "jl15871@stern.nyu.edu"
API_PASSWORD = "b38868cd00c33c33"
API_ENDPOINT = "https://api.dataforseo.com/v3/dataforseo_labs/google/historical_bulk_traffic_estimation/live"

# Target domains
domains = ["covertswarm.com", "redscan.com", "tekkis.com", "ek.co"]

# Initialize result storage
all_results = []

# Loop through each domain
for domain in domains:
    print(f"Fetching data for {domain}...")

    # Create POST payload
    payload = [
        {
            "target": domain,
            "location_name": "United States",
            "language_name": "English",
            "date_from": "2022-12-01",  # Start date
            "date_to": "2024-12-31",    # End date
            "filters": [
                ["keyword_info.search_volume", ">", 0]
            ],
            "limit": 1000
        }
    ]

    # Send POST request
    response = requests.post(API_ENDPOINT, json=payload, auth=HTTPBasicAuth(API_USER, API_PASSWORD))

    # Check for HTTP response status
    if response.status_code == 200:
        data = response.json()
        print(f"Raw API Response for {domain}:\n{data}")  # Debugging

        # Process valid response
        if data.get("status_code") == 20000:
            tasks = data.get("tasks", [])
            for task in tasks:
                # Check if 'result' key exists and is iterable
                result = task.get("result")
                if result is not None and isinstance(result, list): # add a check here before iterating
                    for res in result:
                        for item in res.get("items", []):
                            # Extract data fields
                            keyword_info = {
                                "domain": domain,
                                "keyword": item.get("keyword", "N/A"),
                                "search_volume": item.get("keyword_info", {}).get("search_volume", 0),
                                "competition": item.get("keyword_info", {}).get("competition", 0.0),
                                "competition_level": item.get("keyword_info", {}).get("competition_level", "UNKNOWN"),
                                "cpc": item.get("keyword_info", {}).get("cpc", 0.0),
                                "last_updated_time": item.get("keyword_info", {}).get("last_updated_time", "N/A"),
                                "organic_pos_1": item.get("metrics", {}).get("organic", {}).get("pos_1", 0),
                                "organic_etv": item.get("metrics", {}).get("organic", {}).get("etv", 0.0),
                                "month": item.get("month", "N/A"),
                                "year": item.get("year", "N/A"),
                            }
                            all_results.append(keyword_info)
                else:
                    print(f"Warning: 'result' key is missing or not a list for domain: {domain}")

        else:
            print(f"Error fetching data for {domain}: {data.get('status_message')}")
    else:
        print(f"HTTP Error {response.status_code} for domain: {domain}")

    # Pause between requests to avoid hitting rate limits
    time.sleep(1)

# Convert results to a pandas DataFrame
df = pd.DataFrame(all_results)

# Save results to CSV
csv_filename = "historical_traffic_estimation.csv"
df.to_csv(csv_filename, index=False)
print(f"Keyword data saved to {csv_filename}")