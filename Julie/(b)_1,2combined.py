# -*- coding: utf-8 -*-
"""(b) 1,2combined

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16lOaqLaXIYLfNcP61aMQYl7ind_Eb-ai
"""

#1. Historical Search Volume - CSV file
import os
import sys
import zipfile
import pandas as pd
from google.colab import files

# Step 1: Upload the `python_Client` zip file
print("Please upload the `python_Client` zip file.")
uploaded = files.upload()  # Prompt for file upload

# Step 2: Automatically detect the uploaded file
if not uploaded:
    print("Error: No file uploaded.")
    sys.exit(1)

# Get the uploaded file name dynamically
zip_file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {zip_file_name}")

# Step 3: Define the extraction folder
extracted_folder = "python_Client_extracted"

# Step 4: Unzip the uploaded file
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

# Verify the extracted content
if not os.path.exists(os.path.join(extracted_folder, "client.py")):
    print(f"Error: client.py not found in {extracted_folder}")
    sys.exit(1)

# Step 5: Append the extracted folder to Python's module search path
sys.path.append(extracted_folder)
try:
    from client import RestClient
    print("Client module imported successfully!")
except ModuleNotFoundError as e:
    print(f"Error importing client module: {e}")
    sys.exit(1)

# Step 6: Initialize the DataForSeo client
client = RestClient("jl15871@stern.nyu.edu", "b38868cd00c33c33")  # Replace with your actual credentials

# Step 7: Extract relevant keywords using Google Trends API
initial_keywords = [
    "ethical hacking",
    "Joohyang"
]

google_trends_post_data = dict()
google_trends_post_data[len(google_trends_post_data)] = dict(
    location_name="United States",
    date_from="2022-12-05",  # Broader date range
    date_to="2024-12-04",
    keywords=initial_keywords
)

# Step 8: Send the POST request to Google Trends API
google_trends_response = client.post("/v3/keywords_data/google_trends/explore/live", google_trends_post_data)

# Debug the API response
print("Google Trends API Response:")
print(google_trends_response)

# Initialize a variable to store the relevant keywords
relevant_keywords_from_google_trend = []

if google_trends_response["status_code"] == 20000:
    # Extract relevant keywords from the Google Trends response
    tasks = google_trends_response.get("tasks", [])
    for task in tasks:
        result = task.get("result", [])
        for res in result:
            for item in res.get("items", []):
                if "keywords" in item:
                    relevant_keywords_from_google_trend.extend(item["keywords"])

    # Deduplicate keywords
    relevant_keywords_from_google_trend = list(set(relevant_keywords_from_google_trend))
    print(f"Extracted {len(relevant_keywords_from_google_trend)} relevant keywords from Google Trends:")
    print(relevant_keywords_from_google_trend)

    # Check if we have relevant keywords before proceeding
    if not relevant_keywords_from_google_trend:
        print("No relevant keywords found. Exiting.")
        sys.exit(0)

    # Step 9: Fetch detailed keyword metrics using Historical Search Volume API
    historical_post_data = dict()
    historical_post_data[len(historical_post_data)] = dict(
        keywords=relevant_keywords_from_google_trend,
        location_name="United States",
        language_name="English"
    )
    historical_response = client.post("/v3/dataforseo_labs/google/historical_search_volume/live", historical_post_data)

    # Debug the API response
    print("Historical Search Volume API Response:")
    print(historical_response)

    if historical_response["status_code"] == 20000:
        # Extract data from Historical Search Volume response
        tasks = historical_response.get("tasks", [])
        detailed_data = []
        for task in tasks:
            result = task.get("result")
            if result is None:
                print(f"No data found for historical task: {task}")
                continue
            for res in result:
                for item in res.get("items", []):
                    keyword_info = item.get("keyword_info", {})
                    keyword_data = {
                        "Keyword": item.get("keyword", ""),
                        "Competition": keyword_info.get("competition", "N/A"),
                        "Competition Level": keyword_info.get("competition_level", "N/A"),
                        "CPC": f"${keyword_info.get('cpc', 0):.2f}",
                        "Search Volume": keyword_info.get("search_volume", 0),
                        "Low Top of Page Bid": f"${keyword_info.get('low_top_of_page_bid', 0):.2f}",
                        "High Top of Page Bid": f"${keyword_info.get('high_top_of_page_bid', 0):.2f}",
                        # Convert categories to strings
                        "Categories": ", ".join(map(str, keyword_info.get("categories", [])))
                    }

                    # Add monthly search volume data
                    for monthly_data in keyword_info.get("monthly_searches", []):
                        month_key = f"{monthly_data['year']}-{monthly_data['month']:02}"
                        keyword_data[month_key] = monthly_data.get("search_volume", 0)

                    detailed_data.append(keyword_data)

        # Step 10: Save final data to CSV
        if detailed_data:
            csv_filename = "keyword_metrics_with_monthly_data.csv"
            df = pd.DataFrame(detailed_data)
            df.to_csv(csv_filename, index=False)
            print(f"Keyword metrics with monthly data saved to {csv_filename}")

            # Auto-download the CSV file
            files.download(csv_filename)
        else:
            print("No data found in the Historical Search Volume API response.")
    else:
        print("Error with Historical Search Volume API. Code: %d Message: %s" %
              (historical_response["status_code"], historical_response["status_message"]))
else:
    print("Error with Google Trends API. Code: %d Message: %s" %
          (google_trends_response["status_code"], google_trends_response["status_message"]))

#Final Ver
# Install necessary libraries
!pip install pandas
!pip install openai
!pip install google-colab

import os
import sys
import zipfile
import pandas as pd
from google.colab import files

# Step 1: Upload the `python_Client` zip file
print("Please upload the `python_Client` zip file.")
uploaded = files.upload()  # Prompt for file upload

# Step 2: Automatically detect the uploaded file
if not uploaded:
    print("Error: No file uploaded.")
    sys.exit(1)

# Get the uploaded file name dynamically
zip_file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {zip_file_name}")

# Step 3: Define the extraction folder
extracted_folder = "python_Client_extracted"

# Step 4: Unzip the uploaded file
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

# Verify the extracted content
if not os.path.exists(os.path.join(extracted_folder, "client.py")):
    print(f"Error: client.py not found in {extracted_folder}")
    sys.exit(1)

# Step 5: Append the extracted folder to Python's module search path
sys.path.append(extracted_folder)
try:
    from client import RestClient
    print("Client module imported successfully!")
except ModuleNotFoundError as e:
    print(f"Error importing client module: {e}")
    sys.exit(1)

# Step 6: Initialize the DataForSeo client
client = RestClient("jl15871@stern.nyu.edu", "b38868cd00c33c33")  # Replace with your actual credentials

# Step 7: Use the provided 1000 keywords
keywords_list = [
    # Paste the full 1000 keywords here
    "Access control", "Advanced persistent threat", "Application security", "Authentication",
    "Authorization", "Behavioral analytics", "Black hat hacking", "Cloud computing security",
    "Cloud security", "Cryptography", "Cyber attack", "Cyber attacks", "Cyber defense",
    "Cyber hygiene", "Cyber incident", "Cyber incident response", "Cyber insurance", "Cyber law",
    "Cyber resilience", "Cyber risk", "Cyber risk management", "Cyber threat",
    "Cyber threat intelligence", "Cyber threats", "Cyber warfare", "Cybercrime",
    "Cybercrime prevention", "Cybersecurity", "Cybersecurity acquisitions",
    "Cybersecurity administrators", "Cybersecurity advancements", "Cybersecurity alliances",
    "Cybersecurity analysis", "Cybersecurity analysts", "Cybersecurity analytics",
    "Cybersecurity applications", "Cybersecurity architects", "Cybersecurity architecture",
    "Cybersecurity architecture assessment", "Cybersecurity architecture design",
    "Cybersecurity architecture design services", "Cybersecurity articles",
    "Cybersecurity assessment", "Cybersecurity assessment and evaluation",
    "Cybersecurity assessment services", "Cybersecurity assessment services provider",
    "Cybersecurity assessment services services", "Cybersecurity assessment tools",
    "Cybersecurity assessments", "Cybersecurity assessments services",
    "Cybersecurity attacks", "Cybersecurity audit", "Cybersecurity audit services",
    "Cybersecurity audit services provider", "Cybersecurity audit services services",
    "Cybersecurity audits", "Cybersecurity audits services", "Cybersecurity awareness",
    "Cybersecurity awareness programs", "Cybersecurity awareness programs provider",
    "Cybersecurity awareness training", "Cybersecurity awareness training programs",
    "Cybersecurity awareness training programs services", "Cybersecurity best practices",
    "Cybersecurity best practices implementation", "Cybersecurity best practices implementation services",
    "Cybersecurity best practices services", "Cybersecurity blogs", "Cybersecurity breach",
    "Cybersecurity breach prevention", "Cybersecurity breach response", "Cybersecurity breaches",
    "Cybersecurity careers", "Cybersecurity certification", "Cybersecurity certification programs",
    "Cybersecurity certifications", "Cybersecurity certifications programs",
    "Cybersecurity certifications programs services", "Cybersecurity certifications services",
    "Cybersecurity challenges", "Cybersecurity collaboration", "Cybersecurity collaborations",
    "Cybersecurity communities", "Cybersecurity community", "Cybersecurity companies",
    "Cybersecurity compliance", "Cybersecurity compliance and regulations",
    "Cybersecurity compliance assessment", "Cybersecurity compliance audits",
    "Cybersecurity compliance management", "Cybersecurity compliance management solutions",
    "Cybersecurity compliance services", "Cybersecurity compliance services services",
    "Cybersecurity conferences", "Cybersecurity consultants", "Cybersecurity consulting",
    "Cybersecurity consulting and advisory", "Cybersecurity consulting and advisory services",
    "Cybersecurity consulting companies", "Cybersecurity consulting firm",
    "Cybersecurity consulting firms", "Cybersecurity consulting services",
    "Cybersecurity consulting services provider", "Cybersecurity consulting services services",
    "Cybersecurity controls", "Cybersecurity controls and safeguards",
    "Cybersecurity controls and safeguards implementation", "Cybersecurity controls assessment",
    "Cybersecurity controls implementation", "Cybersecurity courses", "Cybersecurity defense",
    "Cybersecurity defenses", "Cybersecurity detection", "Cybersecurity detection and response",
    "Cybersecurity developers", "Cybersecurity development", "Cybersecurity devices",
    "Cybersecurity directors", "Cybersecurity distributors", "Cybersecurity education",
    "Cybersecurity educators", "Cybersecurity engineers", "Cybersecurity evaluation",
    "Cybersecurity events", "Cybersecurity experts", "Cybersecurity firms",
    "Cybersecurity forensics", "Cybersecurity forums", "Cybersecurity framework",
    "Cybersecurity frameworks", "Cybersecurity frameworks implementation",
    "Cybersecurity funding", "Cybersecurity governance", "Cybersecurity governance framework",
    "Cybersecurity governance framework development", "Cybersecurity governance framework development services",
    "Cybersecurity governance frameworks", "Cybersecurity grants", "Cybersecurity guidelines",
    "Cybersecurity hardware", "Cybersecurity implementation", "Cybersecurity incident",
    "Cybersecurity incident analysis", "Cybersecurity incident communication",
    "Cybersecurity incident detection", "Cybersecurity incident documentation",
    "Cybersecurity incident escalation", "Cybersecurity incident handling",
    "Cybersecurity incident investigation", "Cybersecurity incident management",
    "Cybersecurity incident management plan", "Cybersecurity incident management services",
    "Cybersecurity incident mitigation", "Cybersecurity incident prevention",
    "Cybersecurity incident recovery", "Cybersecurity incident reporting",
    "Cybersecurity incident response", "Cybersecurity incident response best practices",
    "Cybersecurity incident response communication plan", "Cybersecurity incident response exercise",
    "Cybersecurity incident response framework", "Cybersecurity incident response guidelines",
    "Cybersecurity incident response management", "Cybersecurity incident response plan",
    "Cybersecurity incident response plan example", "Cybersecurity incident response plan example pdf",
    "Cybersecurity incident response plan template", "Cybersecurity incident response planning",
    "Cybersecurity incident response planning services", "Cybersecurity incident response playbook",
    "Cybersecurity incident response policy", "Cybersecurity incident response procedures",
    "Cybersecurity incident response process", "Cybersecurity incident response services",
    "Cybersecurity incident response services services", "Cybersecurity incident response tabletop exercise",
    "Cybersecurity incident response team", "Cybersecurity incident response team roles and responsibilities",
    "Cybersecurity incident response training", "Cybersecurity incident response training program",
    "Cybersecurity incidents", "Cybersecurity industry", "Cybersecurity infrastructure",
    "Cybersecurity innovations", "Cybersecurity integrators", "Cybersecurity intelligence",
    "Cybersecurity internships", "Cybersecurity investments", "Cybersecurity jobs",
    "Cybersecurity joint ventures", "Cybersecurity laws", "Cybersecurity leaders",
    "Cybersecurity leadership", "Cybersecurity management", "Cybersecurity management and governance",
    "Cybersecurity management and governance framework", "Cybersecurity management services",
    "Cybersecurity management solutions", "Cybersecurity management solutions provider",
    "Cybersecurity managers", "Cybersecurity manufacturers", "Cybersecurity market",
    "Cybersecurity measures", "Cybersecurity measures evaluation", "Cybersecurity measures implementation",
    "Cybersecurity measures implementation provider", "Cybersecurity measures services",
    "Cybersecurity mergers", "Cybersecurity mitigation", "Cybersecurity mitigation strategies",
    "Cybersecurity monitoring", "Cybersecurity monitoring services", "Cybersecurity monitoring services provider",
    "Cybersecurity networks", "Cybersecurity news", "Cybersecurity offense",
    "Cybersecurity officers", "Cybersecurity operations", "Cybersecurity operations center",
    "Cybersecurity operations management", "Cybersecurity operations management services",
    "Cybersecurity operations management solutions", "Cybersecurity opportunities",
    "Cybersecurity organizations", "Cybersecurity partners", "Cybersecurity partnerships",
    "Cybersecurity penetration testing", "Cybersecurity planning", "Cybersecurity platforms",
    "Cybersecurity policies", "Cybersecurity policies and procedures",
    "Cybersecurity policies and procedures implementation", "Cybersecurity policies development",
    "Cybersecurity policy", "Cybersecurity policy development", "Cybersecurity policy development services",
    "Cybersecurity policy development services services", "Cybersecurity policy implementation",
    "Cybersecurity practices", "Cybersecurity prevention", "Cybersecurity procedures",
    "Cybersecurity products", "Cybersecurity professionals", "Cybersecurity program",
    "Cybersecurity protection", "Cybersecurity protection services", "Cybersecurity protection services provider",
    "Cybersecurity protocols", "Cybersecurity protocols development", "Cybersecurity protocols services",
    "Cybersecurity providers", "Cybersecurity publications", "Cybersecurity recovery",
    "Cybersecurity recovery planning", "Cybersecurity regulations", "Cybersecurity requirements",
    "Cybersecurity research", "Cybersecurity researchers", "Cybersecurity resellers",
    "Cybersecurity resilience", "Cybersecurity resilience building", "Cybersecurity resources",
    "Cybersecurity risk", "Cybersecurity risk analysis", "Cybersecurity risk assessment",
    "Cybersecurity risk assessment and management", "Cybersecurity risk assessment and mitigation",
    "Cybersecurity risk assessment audits", "Cybersecurity risk assessment best practices",
    "Cybersecurity risk assessment breaches", "Cybersecurity risk assessment certifications",
    "Cybersecurity risk assessment checklist", "Cybersecurity risk assessment compliance",
    "Cybersecurity risk assessment consulting", "Cybersecurity risk assessment controls",
    "Cybersecurity risk assessment defense", "Cybersecurity risk assessment framework",
    "Cybersecurity risk assessment framework nist", "Cybersecurity risk assessment frameworks",
    "Cybersecurity risk assessment guidelines", "Cybersecurity risk assessment matrix",
    "Cybersecurity risk assessment measures", "Cybersecurity risk assessment methodologies",
    "Cybersecurity risk assessment methodology", "Cybersecurity risk assessment methodology framework",
    "Cybersecurity risk assessment plan", "Cybersecurity risk assessment process",
    "Cybersecurity risk assessment processes", "Cybersecurity risk assessment programs",
    "Cybersecurity risk assessment protection", "Cybersecurity risk assessment providers",
    "Cybersecurity risk assessment questionnaire", "Cybersecurity risk assessment regulations",
    "Cybersecurity risk assessment report", "Cybersecurity risk assessment services",
    "Cybersecurity risk assessment software", "Cybersecurity risk assessment solutions",
    "Cybersecurity risk assessment standards", "Cybersecurity risk assessment techniques",
    "Cybersecurity risk assessment technology", "Cybersecurity risk assessment template",
    "Cybersecurity risk assessment threats", "Cybersecurity risk assessment tool",
    "Cybersecurity risk assessment tools", "Cybersecurity risk assessment training",
    "Cybersecurity risk assessment trends", "Cybersecurity risk controls",
    "Cybersecurity risk framework", "Cybersecurity risk governance", "Cybersecurity risk management",
    "Cybersecurity risk management framework", "Cybersecurity risk management services",
    "Cybersecurity risk management services services", "Cybersecurity risk management solutions",
    "Cybersecurity risk management strategies", "Cybersecurity risk management strategies services",
    "Cybersecurity risk mitigation", "Cybersecurity risk monitoring"
]

# Step 8: Extract data from Google Trends API
google_trends_post_data = dict()
google_trends_post_data[len(google_trends_post_data)] = dict(
    location_name="United States",
    date_from="2022-12-05",
    date_to="2024-12-04",
    keywords=keywords_list
)

google_trends_response = client.post("/v3/keywords_data/google_trends/explore/live", google_trends_post_data)

# Debug the API response
print("Google Trends API Response:")
print(google_trends_response)

# Initialize a variable to store the relevant keywords
relevant_keywords_from_google_trend = []

if google_trends_response["status_code"] == 20000:
    tasks = google_trends_response.get("tasks", [])
    for task in tasks:
        result = task.get("result", [])
        # Check if result is a list before iterating
        if isinstance(result, list): # this checks if result is a list
            for res in result:
                for item in res.get("items", []):
                    if "keywords" in item:
                        relevant_keywords_from_google_trend.extend(item["keywords"])
        else:
            print(f"Warning: 'result' is not a list for task: {task}. Skipping this task.")


    # Deduplicate keywords
    relevant_keywords_from_google_trend = list(set(relevant_keywords_from_google_trend))
    print(f"Extracted {len(relevant_keywords_from_google_trend)} relevant keywords from Google Trends:")
    print(relevant_keywords_from_google_trend)

    if not relevant_keywords_from_google_trend:
        print("No relevant keywords found. Continuing with original keywords.")
        # Instead of exiting, continue with the original keywords_list
        relevant_keywords_from_google_trend = keywords_list


    # Step 9: Fetch detailed keyword metrics using Historical Search Volume API
    historical_post_data = dict()
    historical_post_data[len(historical_post_data)] = dict(
        keywords=relevant_keywords_from_google_trend,
        location_name="United States",
        language_name="English"
    )
    historical_response = client.post("/v3/dataforseo_labs/google/historical_search_volume/live", historical_post_data)

    # Debug the API response
    print("Historical Search Volume API Response:")
    print(historical_response)

    if historical_response["status_code"] == 20000:
        tasks = historical_response.get("tasks", [])
        detailed_data = []
        for task in tasks:
            result = task.get("result")
            if result is None:
                print(f"No data found for historical task: {task}")
                continue
            for res in result:
                for item in res.get("items", []):
                    keyword_info = item.get("keyword_info", {})

                    keyword_data = {
                        "Keyword": item.get("keyword", ""),
                        "Competition": keyword_info.get("competition", "N/A"),
                        "Competition Level": keyword_info.get("competition_level", "N/A"),
                        "CPC": f"${keyword_info.get('cpc', 0) or 0:.2f}",  # Handle None with or 0
                        "Search Volume": keyword_info.get("search_volume", 0),
                        "Low Top of Page Bid": f"${keyword_info.get('low_top_of_page_bid', 0) or 0:.2f}",  # Handle None with or 0
                        "High Top of Page Bid": f"${keyword_info.get('high_top_of_page_bid', 0) or 0:.2f}",  # Handle None with or 0
                        "Categories": ", ".join(map(str, keyword_info.get("categories", [])))
                    }

                    for monthly_data in keyword_info.get("monthly_searches", []):
                        month_key = f"{monthly_data['year']}-{monthly_data['month']:02}"
                        keyword_data[month_key] = monthly_data.get("search_volume", 0)

                    detailed_data.append(keyword_data)

        # Step 10: Save final data to CSV
        if detailed_data:
            csv_filename = "keyword_metrics_with_monthly_data.csv"
            df = pd.DataFrame(detailed_data)
            df.to_csv(csv_filename, index=False)
            print(f"Keyword metrics with monthly data saved to {csv_filename}")

            # Auto-download the CSV file
            files.download(csv_filename)
        else:
            print("No data found in the Historical Search Volume API response.")
    else:
        print("Error with Historical Search Volume API. Code: %d Message: %s" %
              (historical_response["status_code"], historical_response["status_message"]))
else:
    print("Error with Google Trends API. Code: %d Message: %s" %
          (google_trends_response["status_code"], google_trends_response["status_message"]))

#2 Trying to extract the relevant keywords
import os
import sys
import zipfile
import pandas as pd
from google.colab import files

# Step 1: Upload the python_Client zip file
print("Please upload the python_Client zip file.")
uploaded = files.upload()  # Prompt for file upload

# Automatically detect the uploaded file
if not uploaded:
    print("Error: No file uploaded.")
    sys.exit(1)

# Extract the uploaded zip file
zip_file_name = list(uploaded.keys())[0]
print(f"Uploaded file: {zip_file_name}")

extracted_folder = "python_Client_extracted"
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extracted_folder)

# Verify the extracted content
if not os.path.exists(os.path.join(extracted_folder, "client.py")):
    print(f"Error: client.py not found in {extracted_folder}")
    sys.exit(1)

# Add the extracted folder to Python's module search path
sys.path.append(extracted_folder)
try:
    from client import RestClient
    print("Client module imported successfully!")
except ModuleNotFoundError as e:
    print(f"Error importing client module: {e}")
    sys.exit(1)

# Initialize the DataForSeo client
client = RestClient("jl15871@nyu.edu", "1865d702aa3cad47")  # Replace with your actual credentials

# Define the initial keywords
initial_keywords = [
    "samsung",
    "apple"
]

# Prepare the Google Trends API request payload
google_trends_post_data = dict()
google_trends_post_data[len(google_trends_post_data)] = dict(
    location_name="United States",
    date_from="2023-01-01",
    date_to="2024-11-24",
    keywords=initial_keywords
)

# Fetch data from Google Trends API
google_trends_response = client.post("/v3/keywords_data/google_trends/explore/live", google_trends_post_data)

# Debug: Print the API response
print("Google Trends API Response:")
print(google_trends_response)

if google_trends_response["status_code"] == 20000:
    tasks = google_trends_response.get("tasks", [])
    related_topics = []  # List to store related topics

    # Enhanced inspection of the API response
    print("Inspecting API response for related topics...")
    for task in tasks:
        result = task.get("result", [])
        for res in result:
            for item in res.get("items", []):
                if item.get("type") == "google_trends_topics_list":  # Check for related topics list
                    print(f"Found related topics: {item}")
                    for topic in item.get("data", {}).get("top", []):  # Extract "top" related topics
                        related_topics.append({
                            "topic_title": topic.get("topic_title"),
                            "topic_type": topic.get("topic_type"),
                            "value": topic.get("value")
                        })

    if not related_topics:
        print("No data found in `google_trends_topics_list`. Attempting fallback using `google_trends_graph`.")
        # Fallback to other response fields if no related topics are found
        for task in tasks:
            result = task.get("result", [])
            for res in result:
                for item in res.get("items", []):
                    if item.get("type") == "google_trends_graph":
                        related_topics = [{"topic_title": kw, "topic_type": "Keyword", "value": 0} for kw in item.get("keywords", [])]
                        print(f"Using fallback keywords: {related_topics}")
                        break

    if not related_topics:
        print("No relevant topics or keywords found. Exiting.")
        sys.exit(0)

    # Sort topics by value in descending order
    related_topics = sorted(related_topics, key=lambda x: x["value"], reverse=True)
    print("\nSorted Related Topics by Value:")
    for topic in related_topics:
        print(topic)

    # Extract topic titles for fetching keyword data
    relevant_keywords = [topic["topic_title"] for topic in related_topics if topic["topic_title"]]

    # Ensure relevant_keywords is not empty
    if not relevant_keywords:
        print("No relevant keywords extracted. Exiting.")
        sys.exit(0)

    print(f"Relevant keywords extracted: {relevant_keywords}")
else:
    print("Error with Google Trends API. Code: %d Message: %s" %
          (google_trends_response["status_code"], google_trends_response["status_message"]))

#3. Nick ver_was able to take out the strength(value)of each keywords(https://docs.dataforseo.com/v3/keywords_data/google_trends/explore/live/?python)
# Value here means, increase in the search term popularity. indicates the relative increase in the search term popularity within the given timeframe the value is provided in percentage (without the % sign)

# Debug: Print the entire API response for inspection
import json
print("Google Trends API Raw Response:")
print(json.dumps(google_trends_response, indent=4))

if google_trends_response["status_code"] == 20000:
    tasks = google_trends_response.get("tasks", [])
    related_topics = []  # List to store related topics

    # Enhanced inspection of the API response
    print("Inspecting API response for related topics...")
    for task in tasks:
        result = task.get("result", [])
        for res in result:
            for item in res.get("items", []):
                if item.get("type") == "google_trends_topics_list":  # Check for related topics list
                    print(f"Found related topics: {item}")
                    for topic in item.get("data", {}).get("top", []):  # Extract "top" related topics
                        value = topic.get("value")  # Safely extract 'value'
                        if value is not None:
                            related_topics.append({
                                "topic_title": topic.get("topic_title"),
                                "topic_type": topic.get("topic_type"),
                                "value": value
                            })
                        else:
                            print(f"Warning: Missing 'value' for topic: {topic}")

    if not related_topics:
        print("No data found in `google_trends_topics_list`. Attempting fallback using `google_trends_graph`.")
        # Fallback to other response fields if no related topics are found
        for task in tasks:
            result = task.get("result", [])
            for res in result:
                for item in res.get("items", []):
                    if item.get("type") == "google_trends_graph":
                        for kw in item.get("keywords", []):
                            related_topics.append({
                                "topic_title": kw,
                                "topic_type": "Keyword",
                                "value": 0  # Default fallback for graph keywords
                            })
                        print(f"Using fallback keywords: {related_topics}")
                        break

    # Exit if no topics are found
    if not related_topics:
        print("No relevant topics or keywords found. Exiting.")
        sys.exit(0)

    # Sort topics by value in descending order
    related_topics = sorted(related_topics, key=lambda x: x["value"], reverse=True)
    print("\nSorted Related Topics by Value:")
    for topic in related_topics:
        print(topic)

    # Extract topic titles for fetching keyword data
    relevant_keywords = [topic["topic_title"] for topic in related_topics if topic["topic_title"]]

    # Ensure relevant_keywords is not empty
    if not relevant_keywords:
        print("No relevant keywords extracted. Exiting.")
        sys.exit(0)

    print(f"Relevant keywords extracted: {relevant_keywords}")
else:
    print("Error with Google Trends API. Code: %d Message: %s" %
          (google_trends_response["status_code"], google_trends_response["status_message"]))